<?xml version="1.0" encoding="utf-8"?><testsuite errors="0" failures="1" name="pytest" skips="0" tests="6" time="1.782"><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_all_together.py]" time="0.46579980850219727"><system-out>
Testing that running pytest on file test_all_together.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 9, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests0, inifile: 
plugins: logging-2015.11.4, html-1.9.0, harvest-1.0.2.dev1+g167cf48, cov-2.6.0
collected 9 items

test_run_all_tests.py .........

=========================== 9 passed in 0.43 seconds ===========================
</system-out></testcase><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_doc_example.py]" time="0.634268045425415"><failure message="assert 8 == 7
 +  where 7 = &lt;built-in method get of dict object at 0x7f5e0305ea48&gt;(&apos;passed&apos;, 0)
 +    where &lt;built-in method get of dict object at 0x7f5e0305ea48&gt; = {&apos;failed&apos;: 1, &apos;passed&apos;: 7, &apos;seconds&apos;: 58}.get">test_to_run = &apos;test_doc_example.py&apos;
testdir = &lt;Testdir local(&apos;/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1&apos;)&gt;

    @pytest.mark.parametrize(&apos;test_to_run&apos;, test_files, ids=str)
    def test_run_all_tests(test_to_run, testdir):
        &quot;&quot;&quot;
        This is a meta-test. It is executed for each test file in the &apos;tests_raw&apos; folder.
        For each of them, the file is retrieved and the expected test results are read from its first lines.
        Then a dedicated pytest runner is run on this file, and the results are compared with the expected ones.
    
        See https://docs.pytest.org/en/latest/writing_plugins.html
    
        :param test_to_run:
        :param testdir:
        :return:
        &quot;&quot;&quot;
    
        with open(join(tests_raw_folder, test_to_run)) as f:
            # Create a temporary conftest.py file
            # testdir.makeconftest(&quot;&quot;&quot;&quot;&quot;&quot;)
    
            # create a temporary pytest test file
            test_file_contents = f.read()
            testdir.makepyfile(test_file_contents)
    
            # Grab the expected things to check when this is executed
            m = META_REGEX.match(test_file_contents)
            assert m is not None
            asserts_dct_str = m.groupdict()[&apos;asserts_dct&apos;]
            asserts_dct = ast.literal_eval(asserts_dct_str)
    
            # Here we run pytest
            print(&quot;\nTesting that running pytest on file %s results in %s&quot; % (test_to_run, str(asserts_dct)))
            result = testdir.runpytest()  # (&quot;-q&quot;)
    
            # Here we check that everything is ok
            try:
                result.assert_outcomes(**asserts_dct)
            except Exception as e:
                print(&quot;Error while asserting that %s results in %s&quot; % (test_to_run, str(asserts_dct)))
&gt;               six.raise_from(e, e)

/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
&lt;string&gt;:3: in raise_from
    ???
/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:56: in test_run_all_tests
    result.assert_outcomes(**asserts_dct)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_pytest.pytester.RunResult object at 0x7f5e030616d8&gt;, passed = 8
skipped = 0, failed = 0

    def assert_outcomes(self, passed=0, skipped=0, failed=0):
        &quot;&quot;&quot; assert that the specified outcomes appear with the respective
            numbers (0 means it didn&apos;t occur) in the text output from a test run.&quot;&quot;&quot;
        d = self.parseoutcomes()
&gt;       assert passed == d.get(&quot;passed&quot;, 0)
E       assert 8 == 7
E        +  where 7 = &lt;built-in method get of dict object at 0x7f5e0305ea48&gt;(&apos;passed&apos;, 0)
E        +    where &lt;built-in method get of dict object at 0x7f5e0305ea48&gt; = {&apos;failed&apos;: 1, &apos;passed&apos;: 7, &apos;seconds&apos;: 58}.get

/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/_pytest/pytester.py:370: AssertionError</failure><system-out>
Testing that running pytest on file test_doc_example.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 8, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1, inifile: 
plugins: logging-2015.11.4, html-1.9.0, harvest-1.0.2.dev1+g167cf48, cov-2.6.0
collected 8 items

test_run_all_tests.py .......F

=================================== FAILURES ===================================
________________________________ test_synthesis ________________________________

request = &lt;FixtureRequest for &lt;Function &apos;test_synthesis&apos;&gt;&gt;
store = OrderedDict([(&apos;dataset&apos;, OrderedDict([(&apos;test_run_all_tests.py::test_my_app_bench[A-1]&apos;, &apos;my dataset #A&apos;), (&apos;test_run_a...40174878175103}), (&apos;test_run_all_tests.py::test_my_app_bench[C-2]&apos;, ResultsBag:
{&apos;accuracy&apos;: 0.32188819215916387})]))])

    def test_synthesis(request, store):
        &quot;&quot;&quot;
        An example test that retrieves synthesis information about this module
        &quot;&quot;&quot;
        # retrieve the synthesis, merged with the fixture store
        results_dct = get_session_synthesis_dct(request.session, filter=test_synthesis.__module__,
                                                durations_in_ms=True, test_id_format=&apos;function&apos;,
                                                status_details=False, fixture_store=store,
                                                flatten=True, flatten_more=&apos;my_results&apos;)
    
        # print keys and first node details
        print(&quot;\nKeys:\n&quot; + &quot;\n&quot;.join(list(results_dct.keys())))
        print(&quot;\nFirst node:\n&quot; + &quot;\n&quot;.join(repr(k) + &quot;: &quot; + repr(v) for k, v in list(results_dct.values())[0].items()))
    
        # convert to a pandas dataframe
        results_df = pd.DataFrame.from_dict(results_dct, orient=&apos;index&apos;)
&gt;       results_df = results_df.loc[list(results_dct.keys()), :]          # fix rows order

test_run_all_tests.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pandas/core/indexing.py:1472: in __getitem__
    return self._getitem_tuple(key)
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pandas/core/indexing.py:890: in _getitem_tuple
    retval = getattr(retval, self.name)._getitem_axis(key, axis=i)
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pandas/core/indexing.py:1901: in _getitem_axis
    return self._getitem_iterable(key, axis=axis)
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pandas/core/indexing.py:1143: in _getitem_iterable
    self._validate_read_indexer(key, indexer, axis)
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pandas/core/indexing.py:1200: in _validate_read_indexer
    missing = (indexer &lt; 0).sum()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([False, False, False, False, False, False, False]), axis = None
dtype = None, out = None, keepdims = False, initial = &lt;no value&gt;

    def _sum(a, axis=None, dtype=None, out=None, keepdims=False,
             initial=_NoValue):
&gt;       return umr_sum(a, axis, dtype, out, keepdims, initial)
E       TypeError: int() argument must be a string, a bytes-like object or a number, not &apos;_NoValueType&apos;

/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/numpy/core/_methods.py:36: TypeError
----------------------------- Captured stdout call -----------------------------

Keys:
test_my_app_bench[A-1]
test_my_app_bench[A-2]
test_my_app_bench[B-1]
test_my_app_bench[B-2]
test_my_app_bench[C-1]
test_my_app_bench[C-2]
test_basic

First node:
&apos;pytest_obj&apos;: &lt;function test_my_app_bench at 0x7f5e028a39d8&gt;
&apos;status&apos;: &apos;passed&apos;
&apos;duration_ms&apos;: 0.12493133544921875
&apos;algo_param&apos;: 1
&apos;dataset&apos;: &apos;my dataset #A&apos;
&apos;accuracy&apos;: 0.4708697839819317
====================== 1 failed, 7 passed in 0.58 seconds ======================
Error while asserting that test_doc_example.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 8, &apos;failed&apos;: 0}
</system-out></testcase><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_saved_fixture_in_fixture.py]" time="0.04376554489135742"><system-out>
Testing that running pytest on file test_saved_fixture_in_fixture.py results in {&apos;passed&apos;: 2, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests2, inifile: 
plugins: logging-2015.11.4, html-1.9.0, harvest-1.0.2.dev1+g167cf48, cov-2.6.0
collected 2 items

test_run_all_tests.py ..

=========================== 2 passed in 0.02 seconds ===========================
</system-out></testcase><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_results_bags.py]" time="0.36726880073547363"><system-out>
Testing that running pytest on file test_results_bags.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 6, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests3, inifile: 
plugins: logging-2015.11.4, html-1.9.0, harvest-1.0.2.dev1+g167cf48, cov-2.6.0
collected 6 items

test_run_all_tests.py ......

=========================== 6 passed in 0.34 seconds ===========================
</system-out></testcase><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_get_session_results.py]" time="0.09120392799377441"><system-out>
Testing that running pytest on file test_get_session_results.py results in {&apos;skipped&apos;: 1, &apos;passed&apos;: 15, &apos;failed&apos;: 1}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests4, inifile: 
plugins: logging-2015.11.4, html-1.9.0, harvest-1.0.2.dev1+g167cf48, cov-2.6.0
collected 17 items

test_run_all_tests.py ........sF.......

=================================== FAILURES ===================================
_________________________________ test_failing _________________________________

    def test_failing():
&gt;       pytest.fail(&quot;normal, intended failure here&quot;)
E       Failed: normal, intended failure here

test_run_all_tests.py:116: Failed
================ 1 failed, 15 passed, 1 skipped in 0.06 seconds ================
</system-out></testcase><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_saved_fixture_in_global_var.py]" time="0.04484367370605469"><system-out>
Testing that running pytest on file test_saved_fixture_in_global_var.py results in {&apos;passed&apos;: 2, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests5, inifile: 
plugins: logging-2015.11.4, html-1.9.0, harvest-1.0.2.dev1+g167cf48, cov-2.6.0
collected 2 items

test_run_all_tests.py ..

=========================== 2 passed in 0.02 seconds ===========================
</system-out></testcase></testsuite>