<?xml version="1.0" encoding="utf-8"?><testsuite errors="0" failures="1" name="pytest" skips="0" tests="6" time="1.793"><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_all_together.py]" time="0.4760103225708008"><system-out>
Testing that running pytest on file test_all_together.py results in {&apos;failed&apos;: 0, &apos;skipped&apos;: 0, &apos;passed&apos;: 9}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests0, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-0.9.1.dev1+g68250d5, cov-2.6.0
collected 9 items

test_run_all_tests.py .........                                          [100%]

=============================== warnings summary ===============================
test_run_all_tests.py:43
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten at position 0 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&apos;flatten, flatten_more&apos;, [(False, None), (True, None), (True, &apos;my_results&apos;)],
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten_more at position 0 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&apos;flatten, flatten_more&apos;, [(False, None), (True, None), (True, &apos;my_results&apos;)],
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten at position 1 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&apos;flatten, flatten_more&apos;, [(False, None), (True, None), (True, &apos;my_results&apos;)],
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten_more at position 1 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&apos;flatten, flatten_more&apos;, [(False, None), (True, None), (True, &apos;my_results&apos;)],
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten at position 2 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&apos;flatten, flatten_more&apos;, [(False, None), (True, None), (True, &apos;my_results&apos;)],
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten_more at position 2 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&apos;flatten, flatten_more&apos;, [(False, None), (True, None), (True, &apos;my_results&apos;)],

-- Docs: https://docs.pytest.org/en/latest/warnings.html
===================== 9 passed, 6 warnings in 0.42 seconds =====================
</system-out></testcase><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_doc_example.py]" time="0.3924217224121094"><system-out>
Testing that running pytest on file test_doc_example.py results in {&apos;failed&apos;: 0, &apos;skipped&apos;: 0, &apos;passed&apos;: 8}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests1, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-0.9.1.dev1+g68250d5, cov-2.6.0
collected 8 items

test_run_all_tests.py ........                                           [100%]

=========================== 8 passed in 0.34 seconds ===========================
</system-out></testcase><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_saved_fixture_in_fixture.py]" time="0.0813446044921875"><system-out>
Testing that running pytest on file test_saved_fixture_in_fixture.py results in {&apos;failed&apos;: 0, &apos;passed&apos;: 2}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests2, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-0.9.1.dev1+g68250d5, cov-2.6.0
collected 2 items

test_run_all_tests.py ..                                                 [100%]

=========================== 2 passed in 0.03 seconds ===========================
</system-out></testcase><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_results_bags.py]" time="0.4209635257720947"><system-out>
Testing that running pytest on file test_results_bags.py results in {&apos;failed&apos;: 0, &apos;skipped&apos;: 0, &apos;passed&apos;: 6}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests3, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-0.9.1.dev1+g68250d5, cov-2.6.0
collected 6 items

test_run_all_tests.py ......                                             [100%]

=========================== 6 passed in 0.37 seconds ===========================
</system-out></testcase><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_get_session_results.py]" time="0.16143345832824707"><failure message="AssertionError: assert {&apos;error&apos;: 0, &apos;failed&apos;: 2, &apos;passed&apos;: 13, &apos;skipped&apos;: 1, ...} == {&apos;error&apos;: 0, &apos;failed&apos;: 1, &apos;passed&apos;: 11, &apos;skipped&apos;: 1, ...}">test_to_run = &apos;test_get_session_results.py&apos;
testdir = &lt;Testdir local(&apos;/tmp/pytest-of-travis/pytest-0/test_run_all_tests4&apos;)&gt;

    @pytest.mark.parametrize(&apos;test_to_run&apos;, test_files, ids=str)
    def test_run_all_tests(test_to_run, testdir):
        &quot;&quot;&quot;
        This is a meta-test. It is executed for each test file in the &apos;tests_raw&apos; folder.
        For each of them, the file is retrieved and the expected test results are read from its first lines.
        Then a dedicated pytest runner is run on this file, and the results are compared with the expected ones.
    
        See https://docs.pytest.org/en/latest/writing_plugins.html
    
        :param test_to_run:
        :param testdir:
        :return:
        &quot;&quot;&quot;
    
        with open(join(tests_raw_folder, test_to_run)) as f:
            # Create a temporary conftest.py file
            # testdir.makeconftest(&quot;&quot;&quot;&quot;&quot;&quot;)
    
            # create a temporary pytest test file
            test_file_contents = f.read()
            testdir.makepyfile(test_file_contents)
    
            # Grab the expected things to check when this is executed
            m = META_REGEX.match(test_file_contents)
            assert m is not None
            asserts_dct_str = m.groupdict()[&apos;asserts_dct&apos;]
            asserts_dct = ast.literal_eval(asserts_dct_str)
    
            # Here we run pytest
            print(&quot;\nTesting that running pytest on file %s results in %s&quot; % (test_to_run, str(asserts_dct)))
            result = testdir.runpytest()  # (&quot;-q&quot;)
    
            # Here we check that everything is ok
            try:
                result.assert_outcomes(**asserts_dct)
            except Exception as e:
                print(&quot;Error while asserting that %s results in %s&quot; % (test_to_run, str(asserts_dct)))
&gt;               six.raise_from(e, e)

/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
&lt;string&gt;:3: in raise_from
    ???
/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:56: in test_run_all_tests
    result.assert_outcomes(**asserts_dct)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_pytest.pytester.RunResult object at 0x7fe569bce160&gt;, passed = 11
skipped = 1, failed = 1, error = 0, xpassed = 0, xfailed = 0

    def assert_outcomes(
        self, passed=0, skipped=0, failed=0, error=0, xpassed=0, xfailed=0
    ):
        &quot;&quot;&quot;Assert that the specified outcomes appear with the respective
        numbers (0 means it didn&apos;t occur) in the text output from a test run.
    
        &quot;&quot;&quot;
        d = self.parseoutcomes()
        obtained = {
            &quot;passed&quot;: d.get(&quot;passed&quot;, 0),
            &quot;skipped&quot;: d.get(&quot;skipped&quot;, 0),
            &quot;failed&quot;: d.get(&quot;failed&quot;, 0),
            &quot;error&quot;: d.get(&quot;error&quot;, 0),
            &quot;xpassed&quot;: d.get(&quot;xpassed&quot;, 0),
            &quot;xfailed&quot;: d.get(&quot;xfailed&quot;, 0),
        }
        expected = {
            &quot;passed&quot;: passed,
            &quot;skipped&quot;: skipped,
            &quot;failed&quot;: failed,
            &quot;error&quot;: error,
            &quot;xpassed&quot;: xpassed,
            &quot;xfailed&quot;: xfailed,
        }
&gt;       assert obtained == expected
E       AssertionError: assert {&apos;error&apos;: 0, &apos;failed&apos;: 2, &apos;passed&apos;: 13, &apos;skipped&apos;: 1, ...} == {&apos;error&apos;: 0, &apos;failed&apos;: 1, &apos;passed&apos;: 11, &apos;skipped&apos;: 1, ...}

/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/_pytest/pytester.py:444: AssertionError</failure><system-out>
Testing that running pytest on file test_get_session_results.py results in {&apos;failed&apos;: 1, &apos;skipped&apos;: 1, &apos;passed&apos;: 11}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests4, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-0.9.1.dev1+g68250d5, cov-2.6.0
collected 16 items

test_run_all_tests.py ........sF....F.                                   [100%]

=================================== FAILURES ===================================
_________________________________ test_failing _________________________________

    def test_failing():
&gt;       pytest.fail(&quot;normal, intended failure here&quot;)
E       Failed: normal, intended failure here

test_run_all_tests.py:115: Failed
_________________________ test_synthesis_id_formatting _________________________

request = &lt;FixtureRequest for &lt;Function &apos;test_synthesis_id_formatting&apos;&gt;&gt;

    def test_synthesis_id_formatting(request):
        &quot;&quot;&quot;
        Note: we could do this at many other places (hook, teardown of a session-scope fixture...)
    
        Note2: we could provide helper methods in pytest_harvest to perform the code below more easily
        :param request:
        :param store:
        :return:
        &quot;&quot;&quot;
        # Get session synthesis filtered on the test function of interest
        fmt = &apos;function&apos;
        results_dct = get_session_synthesis_dct(request.session, filter=TestX.test_easy, test_id_format=fmt)
        assert list(results_dct.keys())[0] == &apos;test_easy[True]&apos;
    
        fmt = &apos;class&apos;
        results_dct = get_session_synthesis_dct(request.session, filter=TestX.test_easy, test_id_format=fmt)
        assert list(results_dct.keys())[0] == &apos;TestX::()::test_easy[True]&apos;
    
        fmt = &apos;module&apos;
        results_dct = get_session_synthesis_dct(request.session, filter=TestX.test_easy, test_id_format=fmt)
&gt;       assert list(results_dct.keys())[0] == &apos;test_get_session_results.py::TestX::()::test_easy[True]&apos;
E       AssertionError: assert &apos;test_run_all...st_easy[True]&apos; == &apos;test_get_sess...st_easy[True]&apos;
E         - test_run_all_tests.py::TestX::()::test_easy[True]
E         ?      ^^  ^^^^^
E         + test_get_session_results.py::TestX::()::test_easy[True]
E         ?      ^^^^^^^^^^  ^  ++

test_run_all_tests.py:158: AssertionError
=============================== warnings summary ===============================
test_run_all_tests.py::test_foo_synthesis_all_options[duration_in_ms=False-flatten=True]
  /tmp/pytest-of-travis/pytest-0/test_run_all_tests4/test_run_all_tests.py:68: RemovedInPytest4Warning: MarkInfo objects are deprecated as they contain merged marks which are hard to deal with correctly.
  Please use node.get_closest_marker(name) or node.iter_markers(name).
  Docs: https://docs.pytest.org/en/latest/mark.html#updating-code
    expected_keys.update({mark.args[0] for mark in test_foo.parametrize})

test_run_all_tests.py::test_foo_synthesis_all_options[duration_in_ms=True-flatten=True]
  /tmp/pytest-of-travis/pytest-0/test_run_all_tests4/test_run_all_tests.py:68: RemovedInPytest4Warning: MarkInfo objects are deprecated as they contain merged marks which are hard to deal with correctly.
  Please use node.get_closest_marker(name) or node.iter_markers(name).
  Docs: https://docs.pytest.org/en/latest/mark.html#updating-code
    expected_keys.update({mark.args[0] for mark in test_foo.parametrize})

-- Docs: https://docs.pytest.org/en/latest/warnings.html
========== 2 failed, 13 passed, 1 skipped, 2 warnings in 0.11 seconds ==========
Error while asserting that test_get_session_results.py results in {&apos;failed&apos;: 1, &apos;skipped&apos;: 1, &apos;passed&apos;: 11}
</system-out></testcase><testcase classname="pytest_harvest.tests.test_all" file="pytest_harvest/tests/test_all.py" line="21" name="test_run_all_tests[test_saved_fixture_in_global_var.py]" time="0.0802147388458252"><system-out>
Testing that running pytest on file test_saved_fixture_in_global_var.py results in {&apos;failed&apos;: 0, &apos;passed&apos;: 2}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests5, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-0.9.1.dev1+g68250d5, cov-2.6.0
collected 2 items

test_run_all_tests.py ..                                                 [100%]

=========================== 2 passed in 0.03 seconds ===========================
</system-out></testcase></testsuite>