<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title>Test Report</title>
    <link href="assets/style.css" rel="stylesheet" type="text/css"/></head>
  <body onLoad="init()">
    <script>/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this file,
 * You can obtain one at http://mozilla.org/MPL/2.0/. */


function toArray(iter) {
    if (iter === null) {
        return null;
    }
    return Array.prototype.slice.call(iter);
}

function find(selector, elem) {
    if (!elem) {
        elem = document;
    }
    return elem.querySelector(selector);
}

function find_all(selector, elem) {
    if (!elem) {
        elem = document;
    }
    return toArray(elem.querySelectorAll(selector));
}

function sort_column(elem) {
    toggle_sort_states(elem);
    var colIndex = toArray(elem.parentNode.childNodes).indexOf(elem);
    var key;
    if (elem.classList.contains('numeric')) {
        key = key_num;
    } else if (elem.classList.contains('result')) {
        key = key_result;
    } else {
        key = key_alpha;
    }
    sort_table(elem, key(colIndex));
}

function show_all_extras() {
    find_all('.col-result').forEach(show_extras);
}

function hide_all_extras() {
    find_all('.col-result').forEach(hide_extras);
}

function show_extras(colresult_elem) {
    var extras = colresult_elem.parentNode.nextElementSibling;
    var expandcollapse = colresult_elem.firstElementChild;
    extras.classList.remove("collapsed");
    expandcollapse.classList.remove("expander");
    expandcollapse.classList.add("collapser");
}

function hide_extras(colresult_elem) {
    var extras = colresult_elem.parentNode.nextElementSibling;
    var expandcollapse = colresult_elem.firstElementChild;
    extras.classList.add("collapsed");
    expandcollapse.classList.remove("collapser");
    expandcollapse.classList.add("expander");
}

function show_filters() {
    var filter_items = document.getElementsByClassName('filter');
    for (var i = 0; i < filter_items.length; i++)
        filter_items[i].hidden = false;
}

function add_collapse() {
    // Add links for show/hide all
    var resulttable = find('table#results-table');
    var showhideall = document.createElement("p");
    showhideall.innerHTML = '<a href="javascript:show_all_extras()">Show all details</a> / ' +
                            '<a href="javascript:hide_all_extras()">Hide all details</a>';
    resulttable.parentElement.insertBefore(showhideall, resulttable);

    // Add show/hide link to each result
    find_all('.col-result').forEach(function(elem) {
        var collapsed = get_query_parameter('collapsed') || 'Passed';
        var extras = elem.parentNode.nextElementSibling;
        var expandcollapse = document.createElement("span");
        if (collapsed.includes(elem.innerHTML)) {
            extras.classList.add("collapsed");
            expandcollapse.classList.add("expander");
        } else {
            expandcollapse.classList.add("collapser");
        }
        elem.appendChild(expandcollapse);

        elem.addEventListener("click", function(event) {
            if (event.currentTarget.parentNode.nextElementSibling.classList.contains("collapsed")) {
                show_extras(event.currentTarget);
            } else {
                hide_extras(event.currentTarget);
            }
        });
    })
}

function get_query_parameter(name) {
    var match = RegExp('[?&]' + name + '=([^&]*)').exec(window.location.search);
    return match && decodeURIComponent(match[1].replace(/\+/g, ' '));
}

function init () {
    reset_sort_headers();

    add_collapse();

    show_filters();

    toggle_sort_states(find('.initial-sort'));

    find_all('.sortable').forEach(function(elem) {
        elem.addEventListener("click",
                              function(event) {
                                  sort_column(elem);
                              }, false)
    });

};

function sort_table(clicked, key_func) {
    var rows = find_all('.results-table-row');
    var reversed = !clicked.classList.contains('asc');
    var sorted_rows = sort(rows, key_func, reversed);
    /* Whole table is removed here because browsers acts much slower
     * when appending existing elements.
     */
    var thead = document.getElementById("results-table-head");
    document.getElementById('results-table').remove();
    var parent = document.createElement("table");
    parent.id = "results-table";
    parent.appendChild(thead);
    sorted_rows.forEach(function(elem) {
        parent.appendChild(elem);
    });
    document.getElementsByTagName("BODY")[0].appendChild(parent);
}

function sort(items, key_func, reversed) {
    var sort_array = items.map(function(item, i) {
        return [key_func(item), i];
    });
    var multiplier = reversed ? -1 : 1;

    sort_array.sort(function(a, b) {
        var key_a = a[0];
        var key_b = b[0];
        return multiplier * (key_a >= key_b ? 1 : -1);
    });

    return sort_array.map(function(item) {
        var index = item[1];
        return items[index];
    });
}

function key_alpha(col_index) {
    return function(elem) {
        return elem.childNodes[1].childNodes[col_index].firstChild.data.toLowerCase();
    };
}

function key_num(col_index) {
    return function(elem) {
        return parseFloat(elem.childNodes[1].childNodes[col_index].firstChild.data);
    };
}

function key_result(col_index) {
    return function(elem) {
        var strings = ['Error', 'Failed', 'Rerun', 'XFailed', 'XPassed',
                       'Skipped', 'Passed'];
        return strings.indexOf(elem.childNodes[1].childNodes[col_index].firstChild.data);
    };
}

function reset_sort_headers() {
    find_all('.sort-icon').forEach(function(elem) {
        elem.parentNode.removeChild(elem);
    });
    find_all('.sortable').forEach(function(elem) {
        var icon = document.createElement("div");
        icon.className = "sort-icon";
        icon.textContent = "vvv";
        elem.insertBefore(icon, elem.firstChild);
        elem.classList.remove("desc", "active");
        elem.classList.add("asc", "inactive");
    });
}

function toggle_sort_states(elem) {
    //if active, toggle between asc and desc
    if (elem.classList.contains('active')) {
        elem.classList.toggle('asc');
        elem.classList.toggle('desc');
    }

    //if inactive, reset all other functions and add ascending active
    if (elem.classList.contains('inactive')) {
        reset_sort_headers();
        elem.classList.remove('inactive');
        elem.classList.add('active');
    }
}

function is_all_rows_hidden(value) {
  return value.hidden == false;
}

function filter_table(elem) {
    var outcome_att = "data-test-result";
    var outcome = elem.getAttribute(outcome_att);
    class_outcome = outcome + " results-table-row";
    var outcome_rows = document.getElementsByClassName(class_outcome);

    for(var i = 0; i < outcome_rows.length; i++){
        outcome_rows[i].hidden = !elem.checked;
    }

    var rows = find_all('.results-table-row').filter(is_all_rows_hidden);
    var all_rows_hidden = rows.length == 0 ? true : false;
    var not_found_message = document.getElementById("not-found-message");
    not_found_message.hidden = !all_rows_hidden;
}
</script>
    <h1>report.html</h1>
    <p>Report generated on 20-Nov-2018 at 17:41:09 by<a href="https://pypi.python.org/pypi/pytest-html"> pytest-html</a> v1.19.0</p>
    <h2>Environment</h2>
    <table id="environment">
      <tr>
        <td>CI</td>
        <td>true</td></tr>
      <tr>
        <td>JAVA_HOME</td>
        <td>/usr/lib/jvm/java-8-oracle</td></tr>
      <tr>
        <td>Packages</td>
        <td>{&apos;pytest&apos;: &apos;3.10.1&apos;, &apos;py&apos;: &apos;1.7.0&apos;, &apos;pluggy&apos;: &apos;0.8.0&apos;}</td></tr>
      <tr>
        <td>Platform</td>
        <td>Linux-4.4.0-101-generic-x86_64-with-debian-jessie-sid</td></tr>
      <tr>
        <td>Plugins</td>
        <td>{&apos;html&apos;: &apos;1.19.0&apos;, &apos;harvest&apos;: &apos;1.0.1.dev2+gf0b83e8&apos;, &apos;cov&apos;: &apos;2.6.0&apos;, &apos;metadata&apos;: &apos;1.7.0&apos;}</td></tr>
      <tr>
        <td>Python</td>
        <td>3.5.6</td></tr>
      <tr>
        <td>TRAVIS_BRANCH</td>
        <td>master</td></tr>
      <tr>
        <td>TRAVIS_BUILD_ID</td>
        <td>457562533</td></tr>
      <tr>
        <td>TRAVIS_BUILD_NUMBER</td>
        <td>52</td></tr>
      <tr>
        <td>TRAVIS_COMMIT</td>
        <td>f0b83e8587a41471b8176e4438d816f775750c32</td></tr>
      <tr>
        <td>TRAVIS_COMMIT_MESSAGE</td>
        <td>added __wrapped__ back under another name in my_decorator</td></tr>
      <tr>
        <td>TRAVIS_COMMIT_RANGE</td>
        <td>ebd1de8800f6...f0b83e8587a4</td></tr>
      <tr>
        <td>TRAVIS_EVENT_TYPE</td>
        <td>push</td></tr>
      <tr>
        <td>TRAVIS_JOB_ID</td>
        <td>457562538</td></tr>
      <tr>
        <td>TRAVIS_JOB_NUMBER</td>
        <td>52.5</td></tr>
      <tr>
        <td>TRAVIS_OS_NAME</td>
        <td>linux</td></tr>
      <tr>
        <td>TRAVIS_PULL_REQUEST</td>
        <td>false</td></tr>
      <tr>
        <td>TRAVIS_REPO_SLUG</td>
        <td>smarie/python-pytest-harvest</td></tr>
      <tr>
        <td>TRAVIS_SUDO</td>
        <td>true</td></tr></table>
    <h2>Summary</h2>
    <p>6 tests ran in 1.91 seconds. </p>
    <p class="filter" hidden="true">(Un)check the boxes to filter the results.</p><input checked="true" class="filter" data-test-result="passed" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="passed">5 passed</span>, <input checked="true" class="filter" data-test-result="skipped" disabled="true" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="skipped">0 skipped</span>, <input checked="true" class="filter" data-test-result="failed" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="failed">1 failed</span>, <input checked="true" class="filter" data-test-result="error" disabled="true" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="error">0 errors</span>, <input checked="true" class="filter" data-test-result="xfailed" disabled="true" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="xfailed">0 expected failures</span>, <input checked="true" class="filter" data-test-result="xpassed" disabled="true" hidden="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="xpassed">0 unexpected passes</span>
    <h2>Results</h2>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable result initial-sort" col="result">Result</th>
          <th class="sortable" col="name">Test</th>
          <th class="sortable numeric" col="duration">Duration</th>
          <th>Links</th></tr>
        <tr hidden="true" id="not-found-message">
          <th colspan="4">No results found. Try to check the filters</th></tr></thead>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">pytest_harvest/tests/test_all.py::test_run_all_tests[test_doc_example.py]</td>
          <td class="col-duration">0.62</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">test_to_run = &#x27;test_doc_example.py&#x27;<br/>testdir = &lt;Testdir local(&#x27;/tmp/pytest-of-travis/pytest-0/test_run_all_tests1&#x27;)&gt;<br/><br/>    @pytest.mark.parametrize(&#x27;test_to_run&#x27;, test_files, ids=str)<br/>    def test_run_all_tests(test_to_run, testdir):<br/>        &quot;&quot;&quot;<br/>        This is a meta-test. It is executed for each test file in the &#x27;tests_raw&#x27; folder.<br/>        For each of them, the file is retrieved and the expected test results are read from its first lines.<br/>        Then a dedicated pytest runner is run on this file, and the results are compared with the expected ones.<br/>    <br/>        See https://docs.pytest.org/en/latest/writing_plugins.html<br/>    <br/>        :param test_to_run:<br/>        :param testdir:<br/>        :return:<br/>        &quot;&quot;&quot;<br/>    <br/>        with open(join(tests_raw_folder, test_to_run)) as f:<br/>            # Create a temporary conftest.py file<br/>            # testdir.makeconftest(&quot;&quot;&quot;&quot;&quot;&quot;)<br/>    <br/>            # create a temporary pytest test file<br/>            test_file_contents = f.read()<br/>            testdir.makepyfile(test_file_contents)<br/>    <br/>            # Grab the expected things to check when this is executed<br/>            m = META_REGEX.match(test_file_contents)<br/>            assert m is not None<br/>            asserts_dct_str = m.groupdict()[&#x27;asserts_dct&#x27;]<br/>            asserts_dct = ast.literal_eval(asserts_dct_str)<br/>    <br/>            # Here we run pytest<br/>            print(&quot;\nTesting that running pytest on file %s results in %s&quot; % (test_to_run, str(asserts_dct)))<br/>            result = testdir.runpytest()  # (&quot;-q&quot;)<br/>    <br/>            # Here we check that everything is ok<br/>            try:<br/>                result.assert_outcomes(**asserts_dct)<br/>            except Exception as e:<br/>                print(&quot;Error while asserting that %s results in %s&quot; % (test_to_run, str(asserts_dct)))<br/>&gt;               six.raise_from(e, e)<br/><br/>/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:59: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>&lt;string&gt;:3: in raise_from<br/>    ???<br/>/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:56: in test_run_all_tests<br/>    result.assert_outcomes(**asserts_dct)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;_pytest.pytester.RunResult object at 0x7fe2429d13c8&gt;, passed = 8<br/>skipped = 0, failed = 0, error = 0, xpassed = 0, xfailed = 0<br/><br/>    def assert_outcomes(<br/>        self, passed=0, skipped=0, failed=0, error=0, xpassed=0, xfailed=0<br/>    ):<br/>        &quot;&quot;&quot;Assert that the specified outcomes appear with the respective<br/>        numbers (0 means it didn&#x27;t occur) in the text output from a test run.<br/>    <br/>        &quot;&quot;&quot;<br/>        d = self.parseoutcomes()<br/>        obtained = {<br/>            &quot;passed&quot;: d.get(&quot;passed&quot;, 0),<br/>            &quot;skipped&quot;: d.get(&quot;skipped&quot;, 0),<br/>            &quot;failed&quot;: d.get(&quot;failed&quot;, 0),<br/>            &quot;error&quot;: d.get(&quot;error&quot;, 0),<br/>            &quot;xpassed&quot;: d.get(&quot;xpassed&quot;, 0),<br/>            &quot;xfailed&quot;: d.get(&quot;xfailed&quot;, 0),<br/>        }<br/>        expected = {<br/>            &quot;passed&quot;: passed,<br/>            &quot;skipped&quot;: skipped,<br/>            &quot;failed&quot;: failed,<br/>            &quot;error&quot;: error,<br/>            &quot;xpassed&quot;: xpassed,<br/>            &quot;xfailed&quot;: xfailed,<br/>        }<br/>&gt;       assert obtained == expected<br/><span class="error">E       AssertionError: assert {&#x27;error&#x27;: 0, &#x27;failed&#x27;: 1, &#x27;passed&#x27;: 7, &#x27;skipped&#x27;: 0, ...} == {&#x27;error&#x27;: 0, &#x27;failed&#x27;: 0, &#x27;passed&#x27;: 8, &#x27;skipped&#x27;: 0, ...}</span><br/><br/>/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/_pytest/pytester.py:444: AssertionError<br/>----------------------------- Captured stdout call -----------------------------<br/>
Testing that running pytest on file test_doc_example.py results in {&#x27;failed&#x27;: 0, &#x27;skipped&#x27;: 0, &#x27;passed&#x27;: 8}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests1, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.1.dev2+gf0b83e8, cov-2.6.0
collected 8 items

test_run_all_tests.py .......F                                           [100%]

=================================== FAILURES ===================================
________________________________ test_synthesis ________________________________

request = &lt;FixtureRequest for &lt;Function &#x27;test_synthesis&#x27;&gt;&gt;
store = OrderedDict([(&#x27;dataset&#x27;, OrderedDict([(&#x27;test_run_all_tests.py::test_my_app_bench[A-1]&#x27;, &#x27;my dataset #A&#x27;), (&#x27;test_run_a...570921129157244}), (&#x27;test_run_all_tests.py::test_my_app_bench[C-2]&#x27;, ResultsBag:
{&#x27;accuracy&#x27;: 0.8223183079279867})]))])

    def test_synthesis(request, store):
        &quot;&quot;&quot;
        An example test that retrieves synthesis information about this module
        &quot;&quot;&quot;
        # retrieve the synthesis, merged with the fixture store
        results_dct = get_session_synthesis_dct(request.session, filter=test_synthesis.__module__,
                                                durations_in_ms=True, test_id_format=&#x27;function&#x27;,
                                                status_details=False, fixture_store=store,
                                                flatten=True, flatten_more=&#x27;my_results&#x27;)
    
        # print keys and first node details
        print(&quot;\nKeys:\n&quot; + &quot;\n&quot;.join(list(results_dct.keys())))
        print(&quot;\nFirst node:\n&quot; + &quot;\n&quot;.join(repr(k) + &quot;: &quot; + repr(v) for k, v in list(results_dct.values())[0].items()))
    
        # convert to a pandas dataframe
        results_df = pd.DataFrame.from_dict(results_dct, orient=&#x27;index&#x27;)
&gt;       results_df = results_df.loc[list(results_dct.keys()), :]          # fix rows order

test_run_all_tests.py:74: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pandas/core/indexing.py:1472: in __getitem__
    return self._getitem_tuple(key)
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pandas/core/indexing.py:890: in _getitem_tuple
    retval = getattr(retval, self.name)._getitem_axis(key, axis=i)
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pandas/core/indexing.py:1901: in _getitem_axis
    return self._getitem_iterable(key, axis=axis)
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pandas/core/indexing.py:1143: in _getitem_iterable
    self._validate_read_indexer(key, indexer, axis)
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pandas/core/indexing.py:1200: in _validate_read_indexer
    missing = (indexer &lt; 0).sum()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([False, False, False, False, False, False, False]), axis = None
dtype = None, out = None, keepdims = False, initial = &lt;no value&gt;

    def _sum(a, axis=None, dtype=None, out=None, keepdims=False,
             initial=_NoValue):
&gt;       return umr_sum(a, axis, dtype, out, keepdims, initial)
E       TypeError: int() argument must be a string, a bytes-like object or a number, not &#x27;_NoValueType&#x27;

/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/numpy/core/_methods.py:36: TypeError
----------------------------- Captured stdout call -----------------------------

Keys:
test_my_app_bench[A-1]
test_my_app_bench[A-2]
test_my_app_bench[B-1]
test_my_app_bench[B-2]
test_my_app_bench[C-1]
test_my_app_bench[C-2]
test_basic

First node:
&#x27;pytest_obj&#x27;: &lt;function test_my_app_bench at 0x7fe242091730&gt;
&#x27;status&#x27;: &#x27;passed&#x27;
&#x27;duration_ms&#x27;: 0.2586841583251953
&#x27;algo_param&#x27;: 1
&#x27;dataset&#x27;: &#x27;my dataset #A&#x27;
&#x27;accuracy&#x27;: 0.010390756514550126
====================== 1 failed, 7 passed in 0.58 seconds ======================
Error while asserting that test_doc_example.py results in {&#x27;failed&#x27;: 0, &#x27;skipped&#x27;: 0, &#x27;passed&#x27;: 8}
</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">pytest_harvest/tests/test_all.py::test_run_all_tests[test_all_together.py]</td>
          <td class="col-duration">0.42</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">----------------------------- Captured stdout call -----------------------------<br/>
Testing that running pytest on file test_all_together.py results in {&#x27;failed&#x27;: 0, &#x27;skipped&#x27;: 0, &#x27;passed&#x27;: 9}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests0, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.1.dev2+gf0b83e8, cov-2.6.0
collected 9 items

test_run_all_tests.py .........                                          [100%]

=============================== warnings summary ===============================
test_run_all_tests.py:43
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten at position 0 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&#x27;flatten, flatten_more&#x27;, [(False, None), (True, None), (True, &#x27;my_results&#x27;)],
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten_more at position 0 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&#x27;flatten, flatten_more&#x27;, [(False, None), (True, None), (True, &#x27;my_results&#x27;)],
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten at position 1 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&#x27;flatten, flatten_more&#x27;, [(False, None), (True, None), (True, &#x27;my_results&#x27;)],
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten_more at position 1 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&#x27;flatten, flatten_more&#x27;, [(False, None), (True, None), (True, &#x27;my_results&#x27;)],
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten at position 2 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&#x27;flatten, flatten_more&#x27;, [(False, None), (True, None), (True, &#x27;my_results&#x27;)],
  test_run_all_tests.py:43: RemovedInPytest4Warning: While trying to determine id of parameter flatten_more at position 2 the following exception was raised:
    IndexError: tuple index out of range
  This warning will be an error error in pytest-4.0.
    @pytest.mark.parametrize(&#x27;flatten, flatten_more&#x27;, [(False, None), (True, None), (True, &#x27;my_results&#x27;)],

-- Docs: https://docs.pytest.org/en/latest/warnings.html
===================== 9 passed, 6 warnings in 0.38 seconds =====================
</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">pytest_harvest/tests/test_all.py::test_run_all_tests[test_saved_fixture_in_fixture.py]</td>
          <td class="col-duration">0.07</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">----------------------------- Captured stdout call -----------------------------<br/>
Testing that running pytest on file test_saved_fixture_in_fixture.py results in {&#x27;failed&#x27;: 0, &#x27;passed&#x27;: 2}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests2, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.1.dev2+gf0b83e8, cov-2.6.0
collected 2 items

test_run_all_tests.py ..                                                 [100%]

=========================== 2 passed in 0.03 seconds ===========================
</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">pytest_harvest/tests/test_all.py::test_run_all_tests[test_results_bags.py]</td>
          <td class="col-duration">0.39</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">----------------------------- Captured stdout call -----------------------------<br/>
Testing that running pytest on file test_results_bags.py results in {&#x27;failed&#x27;: 0, &#x27;skipped&#x27;: 0, &#x27;passed&#x27;: 6}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests3, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.1.dev2+gf0b83e8, cov-2.6.0
collected 6 items

test_run_all_tests.py ......                                             [100%]

=========================== 6 passed in 0.34 seconds ===========================
</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">pytest_harvest/tests/test_all.py::test_run_all_tests[test_get_session_results.py]</td>
          <td class="col-duration">0.14</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">----------------------------- Captured stdout call -----------------------------<br/>
Testing that running pytest on file test_get_session_results.py results in {&#x27;failed&#x27;: 1, &#x27;skipped&#x27;: 1, &#x27;passed&#x27;: 15}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests4, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.1.dev2+gf0b83e8, cov-2.6.0
collected 17 items

test_run_all_tests.py ........sF.......                                  [100%]

=================================== FAILURES ===================================
_________________________________ test_failing _________________________________

    def test_failing():
&gt;       pytest.fail(&quot;normal, intended failure here&quot;)
E       Failed: normal, intended failure here

test_run_all_tests.py:116: Failed
=============================== warnings summary ===============================
test_run_all_tests.py::test_foo_synthesis_all_options[duration_in_ms=False-flatten=True]
  /tmp/pytest-of-travis/pytest-0/test_run_all_tests4/test_run_all_tests.py:69: RemovedInPytest4Warning: MarkInfo objects are deprecated as they contain merged marks which are hard to deal with correctly.
  Please use node.get_closest_marker(name) or node.iter_markers(name).
  Docs: https://docs.pytest.org/en/latest/mark.html#updating-code
    expected_keys.update({mark.args[0] for mark in test_foo.parametrize})

test_run_all_tests.py::test_foo_synthesis_all_options[duration_in_ms=True-flatten=True]
  /tmp/pytest-of-travis/pytest-0/test_run_all_tests4/test_run_all_tests.py:69: RemovedInPytest4Warning: MarkInfo objects are deprecated as they contain merged marks which are hard to deal with correctly.
  Please use node.get_closest_marker(name) or node.iter_markers(name).
  Docs: https://docs.pytest.org/en/latest/mark.html#updating-code
    expected_keys.update({mark.args[0] for mark in test_foo.parametrize})

-- Docs: https://docs.pytest.org/en/latest/warnings.html
========== 1 failed, 15 passed, 1 skipped, 2 warnings in 0.09 seconds ==========
</div></td></tr></tbody>
      <tbody class="passed results-table-row">
        <tr>
          <td class="col-result">Passed</td>
          <td class="col-name">pytest_harvest/tests/test_all.py::test_run_all_tests[test_saved_fixture_in_global_var.py]</td>
          <td class="col-duration">0.07</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="4">
            <div class="log">----------------------------- Captured stdout call -----------------------------<br/>
Testing that running pytest on file test_saved_fixture_in_global_var.py results in {&#x27;failed&#x27;: 0, &#x27;passed&#x27;: 2}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests5, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.1.dev2+gf0b83e8, cov-2.6.0
collected 2 items

test_run_all_tests.py ..                                                 [100%]

=========================== 2 passed in 0.03 seconds ===========================
</div></td></tr></tbody></table></body></html>