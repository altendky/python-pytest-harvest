<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <title>Test Report</title>
    <style>body {
	font-family: Helvetica, Arial, sans-serif;
	font-size: 12px;
	min-width: 1200px;
	color: #999;
}
h2 {
	font-size: 16px;
	color: black;
}

p {
    color: black;
}

a {
	color: #999;
}

table {
	border-collapse: collapse;
}

/******************************
 * SUMMARY INFORMATION
 ******************************/

#environment td {
	padding: 5px;
	border: 1px solid #E6E6E6;
}

#environment tr:nth-child(odd) {
	background-color: #f6f6f6;
}

/******************************
 * TEST RESULT COLORS
 ******************************/
span.passed, .passed .col-result {
	color: green;
}
span.skipped, span.xfailed, span.rerun, .skipped .col-result, .xfailed .col-result, .rerun .col-result {
	color: orange;
}
span.error, span.failed, span.xpassed, .error .col-result, .failed .col-result, .xpassed .col-result  {
	color: red;
}


/******************************
 * RESULTS TABLE
 *
 * 1. Table Layout
 * 2. Extra
 * 3. Sorting items
 *
 ******************************/

/*------------------
 * 1. Table Layout
 *------------------*/

#results-table {
	border: 1px solid #e6e6e6;
	color: #999;
	font-size: 12px;
	width: 100%
}

#results-table th, #results-table td {
	padding: 5px;
	border: 1px solid #E6E6E6;
	text-align: left
}
#results-table th {
	font-weight: bold
}

/*------------------
 * 2. Extra
 *------------------*/

.log:only-child {
	height: inherit
}
.log {
	background-color: #e6e6e6;
	border: 1px solid #e6e6e6;
	color: black;
	display: block;
	font-family: "Courier New", Courier, monospace;
	height: 230px;
	overflow-y: scroll;
	padding: 5px;
	white-space: pre-wrap
}
div.image {
	border: 1px solid #e6e6e6;
	float: right;
	height: 240px;
	margin-left: 5px;
	overflow: hidden;
	width: 320px
}
div.image img {
	width: 320px
}

/*------------------
 * 3. Sorting items
 *------------------*/
.sortable {
	cursor: pointer;
}

.sort-icon {
	font-size: 0px;
	float: left;
	margin-right: 5px;
	margin-top: 5px;
	/*triangle*/
	width: 0;
	height: 0;
	border-left: 8px solid transparent;
	border-right: 8px solid transparent;
}

.inactive .sort-icon {
	/*finish triangle*/
	border-top: 8px solid #E6E6E6;
}

.asc.active .sort-icon {
	/*finish triangle*/
	border-bottom: 8px solid #999;
}

.desc.active .sort-icon {
	/*finish triangle*/
	border-top: 8px solid #999;
}
</style></head>
  <body>
    <script>/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this file,
 * You can obtain one at http://mozilla.org/MPL/2.0/. */


function toArray(iter) {
    if (iter === null) {
        return null;
    }
    return Array.prototype.slice.call(iter);
}

function find(selector, elem) {
    if (!elem) {
        elem = document;
    }
    return elem.querySelector(selector);
}

function find_all(selector, elem) {
    if (!elem) {
        elem = document;
    }
    return toArray(elem.querySelectorAll(selector));
}

function sort_column(elem) {
    toggle_sort_states(elem);
    var colIndex = toArray(elem.parentNode.childNodes).indexOf(elem);
    var key;
    if (elem.classList.contains('numeric')) {
        key = key_num;
    } else if (elem.classList.contains('result')) {
        key = key_result;
    } else {
        key = key_alpha;
    }
    sort_table(elem, key(colIndex));
}

addEventListener("DOMContentLoaded", function() {
    reset_sort_headers();

    sort_column(find('.initial-sort'));

    find_all('.col-links a.image').forEach(function(elem) {
        elem.addEventListener("click",
                              function(event) {
                                  var node = elem;
                                  while (node && !node.classList.contains('results-table-row')) {
                                      node = node.parentNode;
                                  }
                                  if (node != null) {
                                      if (node.nextSibling &&
                                          node.nextSibling.classList.contains("extra")) {
                                          var href = find('.image img', node.nextSibling).src;
                                          window.open(href);
                                      }
                                  }
                                  event.preventDefault();
                              }, false)
    });

    find_all('.image a').forEach(function(elem) {
        elem.addEventListener("click",
                              function(event) {
                                  window.open(find('img', elem).getAttribute('src'));
                                  event.preventDefault();
                              }, false)
    });

    find_all('.sortable').forEach(function(elem) {
        elem.addEventListener("click",
                              function(event) {
                                  sort_column(elem);
                              }, false)
    });

});

function sort_table(clicked, key_func) {
    var rows = find_all('.results-table-row');
    var reversed = !clicked.classList.contains('asc');
    var sorted_rows = sort(rows, key_func, reversed);

    var parent = document.getElementById('results-table');
    sorted_rows.forEach(function(elem) {
        parent.appendChild(elem);
    });
}

function sort(items, key_func, reversed) {
    var sort_array = items.map(function(item, i) {
        return [key_func(item), i];
    });
    var multiplier = reversed ? -1 : 1;

    sort_array.sort(function(a, b) {
        var key_a = a[0];
        var key_b = b[0];
        return multiplier * (key_a >= key_b ? 1 : -1);
    });

    return sort_array.map(function(item) {
        var index = item[1];
        return items[index];
    });
}

function key_alpha(col_index) {
    return function(elem) {
        return elem.childNodes[1].childNodes[col_index].firstChild.data.toLowerCase();
    };
}

function key_num(col_index) {
    return function(elem) {
        return parseFloat(elem.childNodes[1].childNodes[col_index].firstChild.data);
    };
}

function key_result(col_index) {
    return function(elem) {
        var strings = ['Error', 'Failed', 'XFailed', 'XPassed', 'Skipped',
                       'Passed'];
        return strings.indexOf(elem.childNodes[1].childNodes[col_index].firstChild.data);
    };
}

function reset_sort_headers() {
    find_all('.sort-icon').forEach(function(elem) {
        elem.parentNode.removeChild(elem);
    });
    find_all('.sortable').forEach(function(elem) {
        var icon = document.createElement("div");
        icon.className = "sort-icon";
        icon.textContent = "vvv";
        elem.insertBefore(icon, elem.firstChild);
        elem.classList.remove("desc", "active");
        elem.classList.add("asc", "inactive");
    });
}

function toggle_sort_states(elem) {
    //if active, toggle between asc and desc
    if (elem.classList.contains('active')) {
        elem.classList.toggle('asc');
        elem.classList.toggle('desc');
    }

    //if inactive, reset all other functions and add ascending active
    if (elem.classList.contains('inactive')) {
        reset_sort_headers();
        elem.classList.remove('inactive');
        elem.classList.add('active');
    }
}

function is_all_rows_hidden(value) {
  return value.hidden == false;
}

function filter_table(elem) {
    var outcome_att = "data-test-result";
    var outcome = elem.getAttribute(outcome_att);
    class_outcome = outcome + " results-table-row";
    var outcome_rows = document.getElementsByClassName(class_outcome);
   
    for(var i = 0; i < outcome_rows.length; i++){
        outcome_rows[i].hidden = !elem.checked;
    }

    var rows = find_all('.results-table-row').filter(is_all_rows_hidden);
    var all_rows_hidden = rows.length == 0 ? true : false;    
    var not_found_message = document.getElementById("not-found-message");
    not_found_message.hidden = !all_rows_hidden;
}
</script>
    <p>Report generated on 09-Nov-2018 at 17:55:37</p>
    <h2>Environment</h2>
    <table id="environment">
      <tr>
        <td>Platform</td>
        <td>Linux-4.4.0-101-generic-x86_64-with-debian-jessie-sid</td></tr>
      <tr>
        <td>Python</td>
        <td>3.5.6</td></tr></table>
    <h2>Summary</h2>
    <p>4 tests ran in 0.89 seconds. </p>
    <p>(Un)check the boxes to filter the results.</p><input checked="true" data-test-result="passed" disabled="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="passed">0 passed</span> <input checked="true" data-test-result="skipped" disabled="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="skipped">0 skipped</span> <input checked="true" data-test-result="failed" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="failed">4 failed</span> <input checked="true" data-test-result="error" disabled="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="error">0 errors</span> <input checked="true" data-test-result="xfailed" disabled="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="xfailed">0 expected failures</span> <input checked="true" data-test-result="xpassed" disabled="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="xpassed">0 unexpected passes</span> <input checked="true" data-test-result="rerun" disabled="true" name="filter_checkbox" onChange="filter_table(this)" type="checkbox"/><span class="rerun">0 rerun</span> 
    <h2>Results</h2>
    <table id="results-table">
      <thead id="results-table-head">
        <tr>
          <th class="sortable initial-sort result" col="result">Result</th>
          <th class="sortable" col="name">Test</th>
          <th class="sortable numeric" col="duration">Duration</th>
          <th>Links</th></tr>
        <tr hidden="true" id="not-found-message">
          <th colspan="5">No results found. Try to check the filters</th></tr></thead>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">pytest_harvest/tests/test_all.py::test_run_all_tests[test_saved_fixture_in_fixture.py]</td>
          <td class="col-duration">0.07</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="5">
            <div class="log">test_to_run = &#x27;test_saved_fixture_in_fixture.py&#x27;<br/>testdir = &lt;Testdir local(&#x27;/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests0&#x27;)&gt;<br/><br/>    @pytest.mark.parametrize(&#x27;test_to_run&#x27;, test_files, ids=str)<br/>    def test_run_all_tests(test_to_run, testdir):<br/>        &quot;&quot;&quot;<br/>        This is a meta-test. It is executed for each test file in the &#x27;tests_raw&#x27; folder.<br/>        For each of them, the file is retrieved and the expected test results are read from its first lines.<br/>        Then a dedicated pytest runner is run on this file, and the results are compared with the expected ones.<br/>    <br/>        See https://docs.pytest.org/en/latest/writing_plugins.html<br/>    <br/>        :param test_to_run:<br/>        :param testdir:<br/>        :return:<br/>        &quot;&quot;&quot;<br/>    <br/>        with open(join(tests_raw_folder, test_to_run)) as f:<br/>            # Create a temporary conftest.py file<br/>            # testdir.makeconftest(&quot;&quot;&quot;&quot;&quot;&quot;)<br/>    <br/>            # create a temporary pytest test file<br/>            test_file_contents = f.read()<br/>            testdir.makepyfile(test_file_contents)<br/>    <br/>            # Grab the expected things to check when this is executed<br/>            m = META_REGEX.match(test_file_contents)<br/>            assert m is not None<br/>            asserts_dct_str = m.groupdict()[&#x27;asserts_dct&#x27;]<br/>            asserts_dct = ast.literal_eval(asserts_dct_str)<br/>    <br/>            # Here we run pytest<br/>            print(&quot;\nTesting that running pytest on file %s results in %s&quot; % (test_to_run, str(asserts_dct)))<br/>            result = testdir.runpytest()  # (&quot;-q&quot;)<br/>    <br/>            # Here we check that everything is ok<br/>            try:<br/>                result.assert_outcomes(**asserts_dct)<br/>            except Exception as e:<br/>                print(&quot;Error while asserting that %s results in %s&quot; % (test_to_run, str(asserts_dct)))<br/>&gt;               six.raise_from(e, e)<br/><br/>/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:59: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>&lt;string&gt;:3: in raise_from<br/>    ???<br/>/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:56: in test_run_all_tests<br/>    result.assert_outcomes(**asserts_dct)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;_pytest.pytester.RunResult object at 0x7f36215acf98&gt;, passed = 2<br/>skipped = 0, failed = 0<br/><br/>    def assert_outcomes(self, passed=0, skipped=0, failed=0):<br/>        &quot;&quot;&quot; assert that the specified outcomes appear with the respective<br/>            numbers (0 means it didn&#x27;t occur) in the text output from a test run.&quot;&quot;&quot;<br/>        d = self.parseoutcomes()<br/>&gt;       assert passed == d.get(&quot;passed&quot;, 0)<br/><span class="error">E       assert 2 == 0</span><br/><span class="error">E        +  where 0 = &lt;built-in method get of dict object at 0x7f36215afbc8&gt;(&#x27;passed&#x27;, 0)</span><br/><span class="error">E        +    where &lt;built-in method get of dict object at 0x7f36215afbc8&gt; = {&#x27;error&#x27;: 2, &#x27;seconds&#x27;: 2}.get</span><br/><br/>/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/_pytest/pytester.py:370: AssertionError<br/>----------------------------- Captured stdout call -----------------------------<br/>
Testing that running pytest on file test_saved_fixture_in_fixture.py results in {&apos;passed&apos;: 2, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.6.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests0, inifile: 
plugins: logging-2015.11.4, html-1.9.0, harvest-0.5.0, cov-2.6.0
collected 2 items

test_run_all_tests.py EE

==================================== ERRORS ====================================
________________ ERROR at setup of test_foo[0.4936468793763946] ________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @yield_fixture(scope=&apos;session&apos;, autouse=True)
    def store(request):
    
        # yield the store fixture
        store = OrderedDict()
        yield store
    
        # check that this util works
        assert get_fixture_value(request, &apos;store&apos;) == store
    
        # check that the store contains everything
        assert &apos;my_fix&apos; in store
        assert len(store[&apos;my_fix&apos;]) == 2
        assert list(store[&apos;my_fix&apos;].keys()) == [item.nodeid for item in request.session.items
                                                if this_file_name in item.nodeid]
        assert list(store[&apos;my_fix&apos;].values()) == [(&quot;my_fix #%s&quot; % n) for n in unique_numbers]
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests0/test_run_all_tests.py:36
_______________ ERROR at setup of test_foo[0.35569061632584587] ________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @yield_fixture(scope=&apos;session&apos;, autouse=True)
    def store(request):
    
        # yield the store fixture
        store = OrderedDict()
        yield store
    
        # check that this util works
        assert get_fixture_value(request, &apos;store&apos;) == store
    
        # check that the store contains everything
        assert &apos;my_fix&apos; in store
        assert len(store[&apos;my_fix&apos;]) == 2
        assert list(store[&apos;my_fix&apos;].keys()) == [item.nodeid for item in request.session.items
                                                if this_file_name in item.nodeid]
        assert list(store[&apos;my_fix&apos;].values()) == [(&quot;my_fix #%s&quot; % n) for n in unique_numbers]
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests0/test_run_all_tests.py:36
=========================== 2 error in 0.02 seconds ============================
Error while asserting that test_saved_fixture_in_fixture.py results in {&apos;passed&apos;: 2, &apos;failed&apos;: 0}
</div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">pytest_harvest/tests/test_all.py::test_run_all_tests[test_results_bags.py]</td>
          <td class="col-duration">0.09</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="5">
            <div class="log">test_to_run = &#x27;test_results_bags.py&#x27;<br/>testdir = &lt;Testdir local(&#x27;/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1&#x27;)&gt;<br/><br/>    @pytest.mark.parametrize(&#x27;test_to_run&#x27;, test_files, ids=str)<br/>    def test_run_all_tests(test_to_run, testdir):<br/>        &quot;&quot;&quot;<br/>        This is a meta-test. It is executed for each test file in the &#x27;tests_raw&#x27; folder.<br/>        For each of them, the file is retrieved and the expected test results are read from its first lines.<br/>        Then a dedicated pytest runner is run on this file, and the results are compared with the expected ones.<br/>    <br/>        See https://docs.pytest.org/en/latest/writing_plugins.html<br/>    <br/>        :param test_to_run:<br/>        :param testdir:<br/>        :return:<br/>        &quot;&quot;&quot;<br/>    <br/>        with open(join(tests_raw_folder, test_to_run)) as f:<br/>            # Create a temporary conftest.py file<br/>            # testdir.makeconftest(&quot;&quot;&quot;&quot;&quot;&quot;)<br/>    <br/>            # create a temporary pytest test file<br/>            test_file_contents = f.read()<br/>            testdir.makepyfile(test_file_contents)<br/>    <br/>            # Grab the expected things to check when this is executed<br/>            m = META_REGEX.match(test_file_contents)<br/>            assert m is not None<br/>            asserts_dct_str = m.groupdict()[&#x27;asserts_dct&#x27;]<br/>            asserts_dct = ast.literal_eval(asserts_dct_str)<br/>    <br/>            # Here we run pytest<br/>            print(&quot;\nTesting that running pytest on file %s results in %s&quot; % (test_to_run, str(asserts_dct)))<br/>            result = testdir.runpytest()  # (&quot;-q&quot;)<br/>    <br/>            # Here we check that everything is ok<br/>            try:<br/>                result.assert_outcomes(**asserts_dct)<br/>            except Exception as e:<br/>                print(&quot;Error while asserting that %s results in %s&quot; % (test_to_run, str(asserts_dct)))<br/>&gt;               six.raise_from(e, e)<br/><br/>/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:59: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>&lt;string&gt;:3: in raise_from<br/>    ???<br/>/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:56: in test_run_all_tests<br/>    result.assert_outcomes(**asserts_dct)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;_pytest.pytester.RunResult object at 0x7f36215347b8&gt;, passed = 6<br/>skipped = 0, failed = 0<br/><br/>    def assert_outcomes(self, passed=0, skipped=0, failed=0):<br/>        &quot;&quot;&quot; assert that the specified outcomes appear with the respective<br/>            numbers (0 means it didn&#x27;t occur) in the text output from a test run.&quot;&quot;&quot;<br/>        d = self.parseoutcomes()<br/>&gt;       assert passed == d.get(&quot;passed&quot;, 0)<br/><span class="error">E       assert 6 == 0</span><br/><span class="error">E        +  where 0 = &lt;built-in method get of dict object at 0x7f3621531688&gt;(&#x27;passed&#x27;, 0)</span><br/><span class="error">E        +    where &lt;built-in method get of dict object at 0x7f3621531688&gt; = {&#x27;error&#x27;: 6, &#x27;seconds&#x27;: 3}.get</span><br/><br/>/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/_pytest/pytester.py:370: AssertionError<br/>----------------------------- Captured stdout call -----------------------------<br/>
Testing that running pytest on file test_results_bags.py results in {&apos;passed&apos;: 6, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.6.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1, inifile: 
plugins: logging-2015.11.4, html-1.9.0, harvest-0.5.0, cov-2.6.0
collected 6 items

test_run_all_tests.py EEEEEE

==================================== ERRORS ====================================
___________________ ERROR at setup of test_my_app_bench[A-1] ___________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @pytest.fixture(scope=&apos;session&apos;, autouse=True)
    def store(request):
        # setup: init the store
        store = OrderedDict()
        yield store
        # teardown: here you can collect all
        assert len(store[&apos;results_bag&apos;]) == 6
        print(dict(store[&apos;results_bag&apos;]))
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1/test_run_all_tests.py:42
___________________ ERROR at setup of test_my_app_bench[A-2] ___________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @pytest.fixture(scope=&apos;session&apos;, autouse=True)
    def store(request):
        # setup: init the store
        store = OrderedDict()
        yield store
        # teardown: here you can collect all
        assert len(store[&apos;results_bag&apos;]) == 6
        print(dict(store[&apos;results_bag&apos;]))
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1/test_run_all_tests.py:42
___________________ ERROR at setup of test_my_app_bench[B-1] ___________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @pytest.fixture(scope=&apos;session&apos;, autouse=True)
    def store(request):
        # setup: init the store
        store = OrderedDict()
        yield store
        # teardown: here you can collect all
        assert len(store[&apos;results_bag&apos;]) == 6
        print(dict(store[&apos;results_bag&apos;]))
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1/test_run_all_tests.py:42
___________________ ERROR at setup of test_my_app_bench[B-2] ___________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @pytest.fixture(scope=&apos;session&apos;, autouse=True)
    def store(request):
        # setup: init the store
        store = OrderedDict()
        yield store
        # teardown: here you can collect all
        assert len(store[&apos;results_bag&apos;]) == 6
        print(dict(store[&apos;results_bag&apos;]))
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1/test_run_all_tests.py:42
___________________ ERROR at setup of test_my_app_bench[C-1] ___________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @pytest.fixture(scope=&apos;session&apos;, autouse=True)
    def store(request):
        # setup: init the store
        store = OrderedDict()
        yield store
        # teardown: here you can collect all
        assert len(store[&apos;results_bag&apos;]) == 6
        print(dict(store[&apos;results_bag&apos;]))
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1/test_run_all_tests.py:42
___________________ ERROR at setup of test_my_app_bench[C-2] ___________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @pytest.fixture(scope=&apos;session&apos;, autouse=True)
    def store(request):
        # setup: init the store
        store = OrderedDict()
        yield store
        # teardown: here you can collect all
        assert len(store[&apos;results_bag&apos;]) == 6
        print(dict(store[&apos;results_bag&apos;]))
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1/test_run_all_tests.py:42
=========================== 6 error in 0.03 seconds ============================
Error while asserting that test_results_bags.py results in {&apos;passed&apos;: 6, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
</div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">pytest_harvest/tests/test_all.py::test_run_all_tests[test_get_session_results.py]</td>
          <td class="col-duration">0.08</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="5">
            <div class="log">test_to_run = &#x27;test_get_session_results.py&#x27;<br/>testdir = &lt;Testdir local(&#x27;/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests2&#x27;)&gt;<br/><br/>    @pytest.mark.parametrize(&#x27;test_to_run&#x27;, test_files, ids=str)<br/>    def test_run_all_tests(test_to_run, testdir):<br/>        &quot;&quot;&quot;<br/>        This is a meta-test. It is executed for each test file in the &#x27;tests_raw&#x27; folder.<br/>        For each of them, the file is retrieved and the expected test results are read from its first lines.<br/>        Then a dedicated pytest runner is run on this file, and the results are compared with the expected ones.<br/>    <br/>        See https://docs.pytest.org/en/latest/writing_plugins.html<br/>    <br/>        :param test_to_run:<br/>        :param testdir:<br/>        :return:<br/>        &quot;&quot;&quot;<br/>    <br/>        with open(join(tests_raw_folder, test_to_run)) as f:<br/>            # Create a temporary conftest.py file<br/>            # testdir.makeconftest(&quot;&quot;&quot;&quot;&quot;&quot;)<br/>    <br/>            # create a temporary pytest test file<br/>            test_file_contents = f.read()<br/>            testdir.makepyfile(test_file_contents)<br/>    <br/>            # Grab the expected things to check when this is executed<br/>            m = META_REGEX.match(test_file_contents)<br/>            assert m is not None<br/>            asserts_dct_str = m.groupdict()[&#x27;asserts_dct&#x27;]<br/>            asserts_dct = ast.literal_eval(asserts_dct_str)<br/>    <br/>            # Here we run pytest<br/>            print(&quot;\nTesting that running pytest on file %s results in %s&quot; % (test_to_run, str(asserts_dct)))<br/>            result = testdir.runpytest()  # (&quot;-q&quot;)<br/>    <br/>            # Here we check that everything is ok<br/>            try:<br/>                result.assert_outcomes(**asserts_dct)<br/>            except Exception as e:<br/>                print(&quot;Error while asserting that %s results in %s&quot; % (test_to_run, str(asserts_dct)))<br/>&gt;               six.raise_from(e, e)<br/><br/>/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:59: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>&lt;string&gt;:3: in raise_from<br/>    ???<br/>/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:56: in test_run_all_tests<br/>    result.assert_outcomes(**asserts_dct)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;_pytest.pytester.RunResult object at 0x7f3621525a58&gt;, passed = 4<br/>skipped = 1, failed = 1<br/><br/>    def assert_outcomes(self, passed=0, skipped=0, failed=0):<br/>        &quot;&quot;&quot; assert that the specified outcomes appear with the respective<br/>            numbers (0 means it didn&#x27;t occur) in the text output from a test run.&quot;&quot;&quot;<br/>        d = self.parseoutcomes()<br/>&gt;       assert passed == d.get(&quot;passed&quot;, 0)<br/><span class="error">E       assert 4 == 0</span><br/><span class="error">E        +  where 0 = &lt;built-in method get of dict object at 0x7f36213ce308&gt;(&#x27;passed&#x27;, 0)</span><br/><span class="error">E        +    where &lt;built-in method get of dict object at 0x7f36213ce308&gt; = {&#x27;error&#x27;: 6, &#x27;seconds&#x27;: 3}.get</span><br/><br/>/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/_pytest/pytester.py:370: AssertionError<br/>----------------------------- Captured stdout call -----------------------------<br/>
Testing that running pytest on file test_get_session_results.py results in {&apos;passed&apos;: 4, &apos;failed&apos;: 1, &apos;skipped&apos;: 1}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.6.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests2, inifile: 
plugins: logging-2015.11.4, html-1.9.0, harvest-0.5.0, cov-2.6.0
collected 6 items

test_run_all_tests.py EEEEEE

==================================== ERRORS ====================================
_____________________ ERROR at setup of test_foo[1-hello] ______________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @yield_fixture(scope=&apos;session&apos;, autouse=True)
    def make_synthesis(request):
        yield
    
        #  teardown callback
        synth_dct = get_session_synthesis_dct(request.session)
    
        from pprint import pprint
        pprint(dict(synth_dct))
    
        # asserts
        these_tests = [item.nodeid for item in request.session.items if this_file_name in item.nodeid]
    
        # -- first check that synth_dct contains all these test nodes
        missing = set(these_tests) - set(synth_dct.keys())
        assert len(missing) == 0
    
        # compute the parameter values for all tests in order
        params = list(product(fixture_params, test_params))
    
        # -- check that all test foo nodes appear as success and contain the right information
        test_foo_nodes = [nid for nid in these_tests if test_foo.__name__ in nid]
        for i, nodeid in enumerate(test_foo_nodes):
            node_synth_dct = synth_dct[nodeid]
            assert set(node_synth_dct.keys()) == {&apos;pytest_obj&apos;,
                                                  &apos;pytest_status&apos;,
                                                  &apos;pytest_duration&apos;,
                                                  &apos;pytest_status_details&apos;,
                                                  &apos;pytest_params&apos;
                                                  }
            # main test information
            assert node_synth_dct[&apos;pytest_obj&apos;] == test_foo
            assert node_synth_dct[&apos;pytest_status&apos;] == &apos;passed&apos;
            assert node_synth_dct[&apos;pytest_duration&apos;] &gt;= 0
    
            # test status details
            stages = [&apos;setup&apos;, &apos;call&apos;, &apos;teardown&apos;]
            assert set(node_synth_dct[&apos;pytest_status_details&apos;].keys()) == set(stages)
            for step in stages:
                assert len(node_synth_dct[&apos;pytest_status_details&apos;][step]) == 2
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][0] == &apos;passed&apos;
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][1] &gt;= 0
    
            # parameter values
            assert set(node_synth_dct[&apos;pytest_params&apos;].keys()) == {&apos;p&apos;, &apos;a_number_str&apos;}
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;a_number_str&apos;] == params[i][0]
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;p&apos;] == params[i][1]
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests2/test_run_all_tests.py:51
_____________________ ERROR at setup of test_foo[1-world] ______________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @yield_fixture(scope=&apos;session&apos;, autouse=True)
    def make_synthesis(request):
        yield
    
        #  teardown callback
        synth_dct = get_session_synthesis_dct(request.session)
    
        from pprint import pprint
        pprint(dict(synth_dct))
    
        # asserts
        these_tests = [item.nodeid for item in request.session.items if this_file_name in item.nodeid]
    
        # -- first check that synth_dct contains all these test nodes
        missing = set(these_tests) - set(synth_dct.keys())
        assert len(missing) == 0
    
        # compute the parameter values for all tests in order
        params = list(product(fixture_params, test_params))
    
        # -- check that all test foo nodes appear as success and contain the right information
        test_foo_nodes = [nid for nid in these_tests if test_foo.__name__ in nid]
        for i, nodeid in enumerate(test_foo_nodes):
            node_synth_dct = synth_dct[nodeid]
            assert set(node_synth_dct.keys()) == {&apos;pytest_obj&apos;,
                                                  &apos;pytest_status&apos;,
                                                  &apos;pytest_duration&apos;,
                                                  &apos;pytest_status_details&apos;,
                                                  &apos;pytest_params&apos;
                                                  }
            # main test information
            assert node_synth_dct[&apos;pytest_obj&apos;] == test_foo
            assert node_synth_dct[&apos;pytest_status&apos;] == &apos;passed&apos;
            assert node_synth_dct[&apos;pytest_duration&apos;] &gt;= 0
    
            # test status details
            stages = [&apos;setup&apos;, &apos;call&apos;, &apos;teardown&apos;]
            assert set(node_synth_dct[&apos;pytest_status_details&apos;].keys()) == set(stages)
            for step in stages:
                assert len(node_synth_dct[&apos;pytest_status_details&apos;][step]) == 2
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][0] == &apos;passed&apos;
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][1] &gt;= 0
    
            # parameter values
            assert set(node_synth_dct[&apos;pytest_params&apos;].keys()) == {&apos;p&apos;, &apos;a_number_str&apos;}
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;a_number_str&apos;] == params[i][0]
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;p&apos;] == params[i][1]
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests2/test_run_all_tests.py:51
_____________________ ERROR at setup of test_foo[2-hello] ______________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @yield_fixture(scope=&apos;session&apos;, autouse=True)
    def make_synthesis(request):
        yield
    
        #  teardown callback
        synth_dct = get_session_synthesis_dct(request.session)
    
        from pprint import pprint
        pprint(dict(synth_dct))
    
        # asserts
        these_tests = [item.nodeid for item in request.session.items if this_file_name in item.nodeid]
    
        # -- first check that synth_dct contains all these test nodes
        missing = set(these_tests) - set(synth_dct.keys())
        assert len(missing) == 0
    
        # compute the parameter values for all tests in order
        params = list(product(fixture_params, test_params))
    
        # -- check that all test foo nodes appear as success and contain the right information
        test_foo_nodes = [nid for nid in these_tests if test_foo.__name__ in nid]
        for i, nodeid in enumerate(test_foo_nodes):
            node_synth_dct = synth_dct[nodeid]
            assert set(node_synth_dct.keys()) == {&apos;pytest_obj&apos;,
                                                  &apos;pytest_status&apos;,
                                                  &apos;pytest_duration&apos;,
                                                  &apos;pytest_status_details&apos;,
                                                  &apos;pytest_params&apos;
                                                  }
            # main test information
            assert node_synth_dct[&apos;pytest_obj&apos;] == test_foo
            assert node_synth_dct[&apos;pytest_status&apos;] == &apos;passed&apos;
            assert node_synth_dct[&apos;pytest_duration&apos;] &gt;= 0
    
            # test status details
            stages = [&apos;setup&apos;, &apos;call&apos;, &apos;teardown&apos;]
            assert set(node_synth_dct[&apos;pytest_status_details&apos;].keys()) == set(stages)
            for step in stages:
                assert len(node_synth_dct[&apos;pytest_status_details&apos;][step]) == 2
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][0] == &apos;passed&apos;
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][1] &gt;= 0
    
            # parameter values
            assert set(node_synth_dct[&apos;pytest_params&apos;].keys()) == {&apos;p&apos;, &apos;a_number_str&apos;}
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;a_number_str&apos;] == params[i][0]
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;p&apos;] == params[i][1]
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests2/test_run_all_tests.py:51
_____________________ ERROR at setup of test_foo[2-world] ______________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @yield_fixture(scope=&apos;session&apos;, autouse=True)
    def make_synthesis(request):
        yield
    
        #  teardown callback
        synth_dct = get_session_synthesis_dct(request.session)
    
        from pprint import pprint
        pprint(dict(synth_dct))
    
        # asserts
        these_tests = [item.nodeid for item in request.session.items if this_file_name in item.nodeid]
    
        # -- first check that synth_dct contains all these test nodes
        missing = set(these_tests) - set(synth_dct.keys())
        assert len(missing) == 0
    
        # compute the parameter values for all tests in order
        params = list(product(fixture_params, test_params))
    
        # -- check that all test foo nodes appear as success and contain the right information
        test_foo_nodes = [nid for nid in these_tests if test_foo.__name__ in nid]
        for i, nodeid in enumerate(test_foo_nodes):
            node_synth_dct = synth_dct[nodeid]
            assert set(node_synth_dct.keys()) == {&apos;pytest_obj&apos;,
                                                  &apos;pytest_status&apos;,
                                                  &apos;pytest_duration&apos;,
                                                  &apos;pytest_status_details&apos;,
                                                  &apos;pytest_params&apos;
                                                  }
            # main test information
            assert node_synth_dct[&apos;pytest_obj&apos;] == test_foo
            assert node_synth_dct[&apos;pytest_status&apos;] == &apos;passed&apos;
            assert node_synth_dct[&apos;pytest_duration&apos;] &gt;= 0
    
            # test status details
            stages = [&apos;setup&apos;, &apos;call&apos;, &apos;teardown&apos;]
            assert set(node_synth_dct[&apos;pytest_status_details&apos;].keys()) == set(stages)
            for step in stages:
                assert len(node_synth_dct[&apos;pytest_status_details&apos;][step]) == 2
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][0] == &apos;passed&apos;
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][1] &gt;= 0
    
            # parameter values
            assert set(node_synth_dct[&apos;pytest_params&apos;].keys()) == {&apos;p&apos;, &apos;a_number_str&apos;}
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;a_number_str&apos;] == params[i][0]
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;p&apos;] == params[i][1]
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests2/test_run_all_tests.py:51
________________________ ERROR at setup of test_skipped ________________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @yield_fixture(scope=&apos;session&apos;, autouse=True)
    def make_synthesis(request):
        yield
    
        #  teardown callback
        synth_dct = get_session_synthesis_dct(request.session)
    
        from pprint import pprint
        pprint(dict(synth_dct))
    
        # asserts
        these_tests = [item.nodeid for item in request.session.items if this_file_name in item.nodeid]
    
        # -- first check that synth_dct contains all these test nodes
        missing = set(these_tests) - set(synth_dct.keys())
        assert len(missing) == 0
    
        # compute the parameter values for all tests in order
        params = list(product(fixture_params, test_params))
    
        # -- check that all test foo nodes appear as success and contain the right information
        test_foo_nodes = [nid for nid in these_tests if test_foo.__name__ in nid]
        for i, nodeid in enumerate(test_foo_nodes):
            node_synth_dct = synth_dct[nodeid]
            assert set(node_synth_dct.keys()) == {&apos;pytest_obj&apos;,
                                                  &apos;pytest_status&apos;,
                                                  &apos;pytest_duration&apos;,
                                                  &apos;pytest_status_details&apos;,
                                                  &apos;pytest_params&apos;
                                                  }
            # main test information
            assert node_synth_dct[&apos;pytest_obj&apos;] == test_foo
            assert node_synth_dct[&apos;pytest_status&apos;] == &apos;passed&apos;
            assert node_synth_dct[&apos;pytest_duration&apos;] &gt;= 0
    
            # test status details
            stages = [&apos;setup&apos;, &apos;call&apos;, &apos;teardown&apos;]
            assert set(node_synth_dct[&apos;pytest_status_details&apos;].keys()) == set(stages)
            for step in stages:
                assert len(node_synth_dct[&apos;pytest_status_details&apos;][step]) == 2
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][0] == &apos;passed&apos;
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][1] &gt;= 0
    
            # parameter values
            assert set(node_synth_dct[&apos;pytest_params&apos;].keys()) == {&apos;p&apos;, &apos;a_number_str&apos;}
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;a_number_str&apos;] == params[i][0]
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;p&apos;] == params[i][1]
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests2/test_run_all_tests.py:51
________________________ ERROR at setup of test_failing ________________________
pytest.fixture functions cannot use ``yield``. Instead write and return an inner function/generator and let the consumer call and iterate over it.:

    @yield_fixture(scope=&apos;session&apos;, autouse=True)
    def make_synthesis(request):
        yield
    
        #  teardown callback
        synth_dct = get_session_synthesis_dct(request.session)
    
        from pprint import pprint
        pprint(dict(synth_dct))
    
        # asserts
        these_tests = [item.nodeid for item in request.session.items if this_file_name in item.nodeid]
    
        # -- first check that synth_dct contains all these test nodes
        missing = set(these_tests) - set(synth_dct.keys())
        assert len(missing) == 0
    
        # compute the parameter values for all tests in order
        params = list(product(fixture_params, test_params))
    
        # -- check that all test foo nodes appear as success and contain the right information
        test_foo_nodes = [nid for nid in these_tests if test_foo.__name__ in nid]
        for i, nodeid in enumerate(test_foo_nodes):
            node_synth_dct = synth_dct[nodeid]
            assert set(node_synth_dct.keys()) == {&apos;pytest_obj&apos;,
                                                  &apos;pytest_status&apos;,
                                                  &apos;pytest_duration&apos;,
                                                  &apos;pytest_status_details&apos;,
                                                  &apos;pytest_params&apos;
                                                  }
            # main test information
            assert node_synth_dct[&apos;pytest_obj&apos;] == test_foo
            assert node_synth_dct[&apos;pytest_status&apos;] == &apos;passed&apos;
            assert node_synth_dct[&apos;pytest_duration&apos;] &gt;= 0
    
            # test status details
            stages = [&apos;setup&apos;, &apos;call&apos;, &apos;teardown&apos;]
            assert set(node_synth_dct[&apos;pytest_status_details&apos;].keys()) == set(stages)
            for step in stages:
                assert len(node_synth_dct[&apos;pytest_status_details&apos;][step]) == 2
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][0] == &apos;passed&apos;
                assert node_synth_dct[&apos;pytest_status_details&apos;][step][1] &gt;= 0
    
            # parameter values
            assert set(node_synth_dct[&apos;pytest_params&apos;].keys()) == {&apos;p&apos;, &apos;a_number_str&apos;}
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;a_number_str&apos;] == params[i][0]
            assert node_synth_dct[&apos;pytest_params&apos;][&apos;p&apos;] == params[i][1]
/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests2/test_run_all_tests.py:51
=========================== 6 error in 0.03 seconds ============================
Error while asserting that test_get_session_results.py results in {&apos;passed&apos;: 4, &apos;failed&apos;: 1, &apos;skipped&apos;: 1}
</div></td></tr></tbody>
      <tbody class="failed results-table-row">
        <tr>
          <td class="col-result">Failed</td>
          <td class="col-name">pytest_harvest/tests/test_all.py::test_run_all_tests[test_saved_fixture_in_global_var.py]</td>
          <td class="col-duration">0.42</td>
          <td class="col-links"></td></tr>
        <tr>
          <td class="extra" colspan="5">
            <div class="log">test_to_run = &#x27;test_saved_fixture_in_global_var.py&#x27;<br/>testdir = &lt;Testdir local(&#x27;/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests3&#x27;)&gt;<br/><br/>    @pytest.mark.parametrize(&#x27;test_to_run&#x27;, test_files, ids=str)<br/>    def test_run_all_tests(test_to_run, testdir):<br/>        &quot;&quot;&quot;<br/>        This is a meta-test. It is executed for each test file in the &#x27;tests_raw&#x27; folder.<br/>        For each of them, the file is retrieved and the expected test results are read from its first lines.<br/>        Then a dedicated pytest runner is run on this file, and the results are compared with the expected ones.<br/>    <br/>        See https://docs.pytest.org/en/latest/writing_plugins.html<br/>    <br/>        :param test_to_run:<br/>        :param testdir:<br/>        :return:<br/>        &quot;&quot;&quot;<br/>    <br/>        with open(join(tests_raw_folder, test_to_run)) as f:<br/>            # Create a temporary conftest.py file<br/>            # testdir.makeconftest(&quot;&quot;&quot;&quot;&quot;&quot;)<br/>    <br/>            # create a temporary pytest test file<br/>            test_file_contents = f.read()<br/>            testdir.makepyfile(test_file_contents)<br/>    <br/>            # Grab the expected things to check when this is executed<br/>            m = META_REGEX.match(test_file_contents)<br/>            assert m is not None<br/>            asserts_dct_str = m.groupdict()[&#x27;asserts_dct&#x27;]<br/>            asserts_dct = ast.literal_eval(asserts_dct_str)<br/>    <br/>            # Here we run pytest<br/>            print(&quot;\nTesting that running pytest on file %s results in %s&quot; % (test_to_run, str(asserts_dct)))<br/>            result = testdir.runpytest()  # (&quot;-q&quot;)<br/>    <br/>            # Here we check that everything is ok<br/>            try:<br/>                result.assert_outcomes(**asserts_dct)<br/>            except Exception as e:<br/>                print(&quot;Error while asserting that %s results in %s&quot; % (test_to_run, str(asserts_dct)))<br/>&gt;               six.raise_from(e, e)<br/><br/>/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:59: <br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/>&lt;string&gt;:3: in raise_from<br/>    ???<br/>/home/travis/build/smarie/python-pytest-harvest/pytest_harvest/tests/test_all.py:56: in test_run_all_tests<br/>    result.assert_outcomes(**asserts_dct)<br/>_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ <br/><br/>self = &lt;_pytest.pytester.RunResult object at 0x7f36213a5ac8&gt;, passed = 2<br/>skipped = 0, failed = 0<br/><br/>    def assert_outcomes(self, passed=0, skipped=0, failed=0):<br/>        &quot;&quot;&quot; assert that the specified outcomes appear with the respective<br/>            numbers (0 means it didn&#x27;t occur) in the text output from a test run.&quot;&quot;&quot;<br/>        d = self.parseoutcomes()<br/>&gt;       assert passed == d.get(&quot;passed&quot;, 0)<br/><span class="error">E       assert 2 == 0</span><br/><span class="error">E        +  where 0 = &lt;built-in method get of dict object at 0x7f3621370608&gt;(&#x27;passed&#x27;, 0)</span><br/><span class="error">E        +    where &lt;built-in method get of dict object at 0x7f3621370608&gt; = {&#x27;error&#x27;: 3, &#x27;seconds&#x27;: 37}.get</span><br/><br/>/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/_pytest/pytester.py:370: AssertionError<br/>----------------------------- Captured stdout call -----------------------------<br/>
Testing that running pytest on file test_saved_fixture_in_global_var.py results in {&apos;passed&apos;: 2, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.6.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests3, inifile: 
plugins: logging-2015.11.4, html-1.9.0, harvest-0.5.0, cov-2.6.0
collected 2 items

test_run_all_tests.py EEE

==================================== ERRORS ====================================
________________ ERROR at setup of test_foo[0.4400668817289517] ________________

object = &lt;code object my_yield_fix at 0x7f36215725d0, file &quot;&lt;decorator-gen-4&gt;&quot;, line 1&gt;

    def getsource(object):
        &quot;&quot;&quot;Return the text of the source code for an object.
    
        The argument may be a module, class, method, function, traceback, frame,
        or code object.  The source code is returned as a single string.  An
        OSError is raised if the source code cannot be retrieved.&quot;&quot;&quot;
&gt;       lines, lnum = getsourcelines(object)

/home/travis/miniconda/envs/test-environment/lib/python3.5/inspect.py:949: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/travis/miniconda/envs/test-environment/lib/python3.5/inspect.py:936: in getsourcelines
    lines, lnum = findsource(object)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = &lt;code object my_yield_fix at 0x7f36215725d0, file &quot;&lt;decorator-gen-4&gt;&quot;, line 1&gt;

    def findsource(object):
        &quot;&quot;&quot;Return the entire source file and starting line number for an object.
    
        The argument may be a module, class, method, function, traceback, frame,
        or code object.  The source code is returned as a list of all the lines
        in the file and the line number indexes a line in that list.  An OSError
        is raised if the source code cannot be retrieved.&quot;&quot;&quot;
    
        file = getsourcefile(object)
        if file:
            # Invalidate cache if needed.
            linecache.checkcache(file)
        else:
            file = getfile(object)
            # Allow filenames in form of &quot;&lt;something&gt;&quot; to pass through.
            # `doctest` monkeypatches `linecache` module to enable
            # inspection, so let `linecache.getlines` to be called.
            if not (file.startswith(&apos;&lt;&apos;) and file.endswith(&apos;&gt;&apos;)):
                raise OSError(&apos;source code not available&apos;)
    
        module = getmodule(object, file)
        if module:
            lines = linecache.getlines(file, module.__dict__)
        else:
            lines = linecache.getlines(file)
        if not lines:
&gt;           raise OSError(&apos;could not get source code&apos;)
E           OSError: could not get source code

/home/travis/miniconda/envs/test-environment/lib/python3.5/inspect.py:767: OSError
________________ ERROR at setup of test_foo[0.5324695674452917] ________________

object = &lt;code object my_yield_fix at 0x7f36215725d0, file &quot;&lt;decorator-gen-4&gt;&quot;, line 1&gt;

    def getsource(object):
        &quot;&quot;&quot;Return the text of the source code for an object.
    
        The argument may be a module, class, method, function, traceback, frame,
        or code object.  The source code is returned as a single string.  An
        OSError is raised if the source code cannot be retrieved.&quot;&quot;&quot;
&gt;       lines, lnum = getsourcelines(object)

/home/travis/miniconda/envs/test-environment/lib/python3.5/inspect.py:949: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/travis/miniconda/envs/test-environment/lib/python3.5/inspect.py:936: in getsourcelines
    lines, lnum = findsource(object)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = &lt;code object my_yield_fix at 0x7f36215725d0, file &quot;&lt;decorator-gen-4&gt;&quot;, line 1&gt;

    def findsource(object):
        &quot;&quot;&quot;Return the entire source file and starting line number for an object.
    
        The argument may be a module, class, method, function, traceback, frame,
        or code object.  The source code is returned as a list of all the lines
        in the file and the line number indexes a line in that list.  An OSError
        is raised if the source code cannot be retrieved.&quot;&quot;&quot;
    
        file = getsourcefile(object)
        if file:
            # Invalidate cache if needed.
            linecache.checkcache(file)
        else:
            file = getfile(object)
            # Allow filenames in form of &quot;&lt;something&gt;&quot; to pass through.
            # `doctest` monkeypatches `linecache` module to enable
            # inspection, so let `linecache.getlines` to be called.
            if not (file.startswith(&apos;&lt;&apos;) and file.endswith(&apos;&gt;&apos;)):
                raise OSError(&apos;source code not available&apos;)
    
        module = getmodule(object, file)
        if module:
            lines = linecache.getlines(file, module.__dict__)
        else:
            lines = linecache.getlines(file)
        if not lines:
&gt;           raise OSError(&apos;could not get source code&apos;)
E           OSError: could not get source code

/home/travis/miniconda/envs/test-environment/lib/python3.5/inspect.py:767: OSError
______________ ERROR at teardown of test_foo[0.5324695674452917] _______________

&gt;   request.addfinalizer(lambda: final_test(request))

test_run_all_tests.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

request = &lt;SubRequest &apos;register_final_test&apos; for &lt;Function &apos;test_foo[0.4400668817289517]&apos;&gt;&gt;

    def final_test(request):
        &quot;&quot;&quot;This is the &quot;test&quot; that will be called when session ends. We check that the STORE contains everything&quot;&quot;&quot;
        for fixture_name, values in [(&apos;my_fix&apos;, [str(n) for n in unique_numbers]),
                                     (&apos;my_yield_fix&apos;, [12, 12])]:
&gt;           assert fixture_name in STORE
E           assert &apos;my_yield_fix&apos; in OrderedDict([(&apos;my_fix&apos;, OrderedDict([(&apos;test_run_all_tests.py::test_foo[0.4400668817289517]&apos;, &apos;0.4400668817289517&apos;), (&apos;test_run_all_tests.py::test_foo[0.5324695674452917]&apos;, &apos;0.5324695674452917&apos;)]))])

test_run_all_tests.py:52: AssertionError
=========================== 3 error in 0.37 seconds ============================
Error while asserting that test_saved_fixture_in_global_var.py results in {&apos;passed&apos;: 2, &apos;failed&apos;: 0}
</div></td></tr></tbody></table></body></html>